{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "**In this part we focus on tools that help you analyze the content of a text.**\n",
    "\n",
    "**Goals**\n",
    "<ol>\n",
    "    <li> Analyze words using a concordance </li>\n",
    "    <li> Create a dispersion plot </li>\n",
    "    <li> Compare tokens between texts </li>\n",
    "    <li> Map from text to sounds to look at syllable distributions </li>\n",
    "    <li> Stem the data to look at word meanings </li>\n",
    "    <li> Tag the tokens with their part of speech using a tagger and dictionary</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## Setup ##\n",
    "###########\n",
    "\n",
    "## This new notebook is a new environment, so we have to set it up again.\n",
    "\n",
    "# Import packages\n",
    "from nltk import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import string\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import the Walden data we saved from the Text Cleaning chapter\n",
    "walden_tokens = pickle.load( open( \"working/walden_clean_tokens.pkl\", \"rb\" ) )\n",
    "walden_text = pickle.load( open( \"working/walden_text.pkl\", \"rb\" ) )\n",
    "walden_raw = pickle.load( open( \"working/walden_raw.pkl\", \"rb\" ) )\n",
    "shakespeare_tokens = pickle.load( open( \"working/shakespeare_tokens.pkl\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances\n",
    "\n",
    "Concordances align one token across multple contexts so we can see at a glance how the word is being used. This can be very useful for understanding how two similar words might be used differently. We will use a concordance to compare \"wisdom\" and \"knowledge\" in the next code block to see how they work.\n",
    "\n",
    ">The next code block generates concordances for two words that share much of their meaning. If concordances can show differences in their meaning (by their context) then we will know more about how the words are used in Walden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Generate concordances ##\n",
    "###########################\n",
    "\n",
    "# Concordance for \"wisdom\"\n",
    "walden_text.concordance(\"wisdom\")\n",
    "\n",
    "# Add a new line to separate the concordances\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Note that > print(\"\\nConcordance for Knowledge\") < would give the exact same result\n",
    "\n",
    "# Concordance for \"knowledge\"\n",
    "walden_text.concordance(\"knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparing \"knowledge\" and \"wisdom\" in Walden**\n",
    "\n",
    "Maybe:\n",
    "* wisdom seems to be something that can be conveyed through text (recorded, in the Bible) and is a characteristic of humans (perhaps versus animals)\n",
    "* knowledge seems to be easier to qualify--masters have more of it but much happens \"without\" it\n",
    "\n",
    "\n",
    "## Positional similarity\n",
    "It can be difficult to compare concordances as they are displayed above, so it's common to look at positional similarity. This is helpful because similar words often appear in similar contexts. We will look at two functions that exploit this linguistic fact to find patterns in word use.\n",
    "<ul>\n",
    "<li>common_contexts() takes a set of tokens and returns the words that immediately precede and follow all words in the set.\n",
    "\n",
    "<li>similar() takes a token and returns other tokens that appear in the same context as itself.\n",
    "<ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "## Compare common token contexts and token similarity ##\n",
    "########################################################\n",
    "\n",
    "# Print common contexts for 'ice' and 'water'\n",
    "print(\"Common contexts for 'ice' and 'water'\") # Title\n",
    "walden_text.common_contexts([\"ice\", \"water\"])\n",
    "\n",
    "# Print similar tokens for 'water'\n",
    "print(\"\\nTerms similar to 'water'\") # Title\n",
    "walden_text.similar(\"water\")\n",
    "\n",
    "# Print similar tokens for 'ice'\n",
    "print(\"\\nTerms similar to 'ice'\") # Title\n",
    "walden_text.similar(\"ice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparing similarity of \"water\" and \"ice\" in Walden**\n",
    "\n",
    "Based on the *common contexts* it appears that:\n",
    "* both are nouns (follows 'the' and prepositions like 'in' and 'of')\n",
    "* both are at Walden (follows walden)\n",
    "* both are in the other's similar terms\n",
    "* water is similar to 'spring' -- which is when the pond will turn from ice to unfrozen water\n",
    "* ice is more similar to 'birds' -- maybe because ice restricts birds to the parts of the pond without it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "***As you have probably gathered, we can learn a lot about a word just by seeing it across many contexts.*** One special type of context has to do with words that frequently appear adjacent to one another. We say these tokens are collocated, meaning they appear together in the same location. Often these words are compound nouns, proper nouns, and idioms.\n",
    "\n",
    ">**The next code block shows the 100 most frequent collocated token pairs. In other words, adjacent tokens that appear together the most.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "## List frequently collocated tokens ##\n",
    "#######################################\n",
    "\n",
    "# Get the top 100 collocated token pairs for the text\n",
    "# Text() here transforms 'walden_tokens' into an NLTK text object, which is required for collocations()\n",
    "Text( walden_tokens ).collocations(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Let's try to list some of the types of collcated tokens we found:**\n",
    "\n",
    "Proper nouns\n",
    "* New England\n",
    "* John Field\n",
    "* Walden Pond\n",
    "\n",
    "Predicates\n",
    "* answer questions\n",
    "* mastered difficulties\n",
    "* hooting owl\n",
    "\n",
    "Idioms\n",
    "* take place\n",
    "* beaten track\n",
    "* good deal\n",
    "\n",
    "\n",
    "## Unique tokens\n",
    "\n",
    "So far, we have just looked at tokens within Walden and the complete works of Shakespeare. But how do they compare?\n",
    "\n",
    "There are many reasons to think they are different. For one thing, Walden was written over 250 years after Shakespeare's death -- before the first English novel was written! Next we will learn what tokens are unique between these texts.\n",
    "\n",
    ">**The next code block creates two lists of common tokens that are in one text but not the other.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "## List unique tokens in each text, relative to the other ##\n",
    "############################################################\n",
    "\n",
    "# Make sure both texts are lowercase\n",
    "walden_lower = [token.lower() for token in walden_tokens]\n",
    "shakespeare_lower = [token.lower() for token in shakespeare_tokens]\n",
    "\n",
    "# Get the frequency distributions for our tokenized texts\n",
    "walden_common = FreqDist( walden_lower )\n",
    "shakespeare_common = FreqDist( shakespeare_lower )\n",
    "\n",
    "# Get the 1000 most frequent tokens\n",
    "# Save the token only (index 0), leaving the frequency (index 1) behind\n",
    "walden_common = [token[0] for token in walden_common.most_common(1000)]\n",
    "shakespeare_common = [token[0] for token in shakespeare_common.most_common(1000)]\n",
    "\n",
    "# Convert our two lists of the 1000 most frequent tokens into sets, which removes duplicates\n",
    "walden_common = set( walden_common )\n",
    "shakespeare_common = set( shakespeare_common )              \n",
    "\n",
    "# Extract all tokens in Walden that do not appear in Shakespeare\n",
    "walden_special = walden_common.difference( shakespeare_common )\n",
    "\n",
    "# Extract all tokens in Shakespeare that do not appear in Walden\n",
    "shakespeare_special = shakespeare_common.difference( walden_common )\n",
    "\n",
    "# Print a pretty two-column list--ignore the details here\n",
    "print('%-8s%-20s%s' % ('', 'W NOT S', 'S NOT W'))\n",
    "for i, (wald, shak) in enumerate(zip(sorted(walden_special), sorted(shakespeare_special))):\n",
    "    print('%-8s%-20s%s' % (i, wald, shak))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "**What do \"write\", \"writing\", \"written\", \"wrote\", \"writes\", \"writer\", \"rewrite\", \"rewrites\" all have in common?**\n",
    "\n",
    "They come from the same word, 'write', which we call the **stem** of the words in this list. We can programatically reduce each token to its stem using a process called *stemming*. Stemming the list above would result in each token being transformed into 'write'. Depending on what you're counting, you might even stem as part of your text cleaning.\n",
    "\n",
    "> In the next code chunk, we stem each token in Walden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## Stem Walden ##\n",
    "#################\n",
    "\n",
    "# Stem all of the cleaned tokens\n",
    "walden_stems = [PorterStemmer().stem(t) for t in walden_tokens]\n",
    "\n",
    "# Regenerate the frequency distribution across the tokens to extract the most common words\n",
    "stem_frequencies = FreqDist( walden_stems )\n",
    "\n",
    "# Query the frequency distribution to return the n most common words\n",
    "common_stems = stem_frequencies.most_common( 20 )\n",
    "\n",
    "# What forms were removed? We take the differences between the tokens and stems and save them as a list to walden_diff.\n",
    "walden_diff = list(set(walden_tokens) - set(walden_stems))\n",
    "\n",
    "# Just for fun, calculate the percentage of tokens removed\n",
    "# We do this by dividing the number of tokens removed by the total, multiplying by 100, then rounding to the nearest integer using round().\n",
    "walden_diff_percent = round((len(walden_diff) / len(walden_tokens)) * 100)\n",
    "\n",
    "# How many word forms did stemming remove?\n",
    "print(\"We removed\", len(walden_diff), \"word forms (\", walden_diff_percent, \"% of the token count ).\")\n",
    "\n",
    "# What do the tokens look like before stemming?\n",
    "# Only show the first 50 tokens/stems\n",
    "print(\"\\nBefore stemming:\\n\", walden_diff[0:50])\n",
    "\n",
    "print(\"\\nAfter stemming:\\n\", walden_stems[0:50])\n",
    "\n",
    "# alphabetically sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our 'primitive' stemmer did okay, but not great\n",
    "\n",
    "To review our results quickly:\n",
    "\n",
    "**Good stems**\n",
    "* wood\n",
    "* labor\n",
    "* hand\n",
    "\n",
    "**Bad stems**\n",
    "* inquiri (inquiries - es)\n",
    "* obtrud (I have no idea, do you?)\n",
    "* economi (economics - cs)\n",
    "\n",
    "If you are beginning to think you can build a better stemmer, you probably can.\n",
    "\n",
    "For our purposes, mispelled words are ignored, meaning we will have some data loss. Unlike many numerical tasks, text processing often requires custom solutions because of the inherent variability in language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging part of speech\n",
    "\n",
    "**Another way to group tokens together is by part of speech (noun, verb, etc).**\n",
    "\n",
    "To do this, we use the NLTK function `pos_tag()` which looks up each token in a dictionary and retreives the most common part of speech. The common parts of speech are noun (NN), verb (VB), adjective (JJ), and adverb (RB). There are many more types and subtypes that NLTK labels using this function, which you can learn about it by runnning the function `help.upenn_tagset()`.\n",
    "\n",
    ">In the following code block, we tag the part of speech of each stemmed token, convert it to a pandas series, and plot a histogram of the frequency of each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "## Tag the tokens with their part of speech ##\n",
    "##############################################\n",
    "\n",
    "# Tag each token with its part of speech\n",
    "walden_tagged = pos_tag( walden_tokens )\n",
    "\n",
    "# Take only the tags (word is index 0, part of speech tag is index 1)\n",
    "walden_tags = [x[1] for x in walden_tagged]\n",
    "\n",
    "# Convert the list of POS tags to a pandas series for easy plotting\n",
    "walden_tags = pd.Series(walden_tags)\n",
    "\n",
    "walden_tags.value_counts().plot(kind='bar').title.set_text('POS Distribution for Walden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity extraction\n",
    "\n",
    "Say we want to find all of the proper names (NN in this part of speech tag set) for a text. You can imagine that you are building a script to automatically create a page index for a book. If you want to look up all of the pages that mention New York, for example, how would you do so?\n",
    "\n",
    "> In the next code chunk, we will see how well the part of speech tagger finds proper nouns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "## Find all of the tokens labeled proper down ##\n",
    "################################################\n",
    "\n",
    "# Loop all of the tagged tokens and print those where the tag is 'NPP' (= proper noun)\n",
    "\n",
    "# Go tag by tag by indexing them one at a time using a loop\n",
    "for i in range(0, len(walden_tagged)):\n",
    "    \n",
    "    # If the current word's ([i]) tag matches 'NNP', run the nested script\n",
    "    if walden_tagged[i][1] == 'NNP':\n",
    "        \n",
    "        # Print the current word\n",
    "        print( walden_tagged[i][0] )\n",
    "\n",
    "# Save our list of NNPs to compare it with another method later\n",
    "# This save each element in walden_tagged if its tag (position 1 in the list) is equal to 'NNP'\n",
    "nnp_old = [x for x in walden_tagged if x[1] == 'NNP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Not so good, but why?**\n",
    "\n",
    "One way to tell whether a word is a proper noun in written English is capitalization, but we removed that information when we cleaned the text. Luckily for us, we can quickly create a new version of the data that is fairly clean and retains its capitalization.\n",
    "\n",
    "## Regex solution\n",
    "\n",
    "Regex is short for **regular expressions**, which is a way to write general pattern. For example, phone numbers are usually written in the same way: *an area code surrounded by parentheses, a space, three numbers, a hyphen, and four more numbers*. If we write this pattern as a regular expression, then we can programmatically extract every phone number formatted like this in a huge set of text.\n",
    "\n",
    "Have you ever wondered how spammers found your phone number, address, and emails? It's because these kind of data are easy to extract from text posted online or electronically available.\n",
    "\n",
    ">**In the next code chunk, I show two examples of how we can extract strings that match different regex patterns. The patterns are not as complicated as a phone number, but you can see the building blocks of this method.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## Setup ##\n",
    "###########\n",
    "\n",
    "# Import regex package\n",
    "import re \n",
    "\n",
    "# Import the raw text of Walden, replacing all new lines with a space, to make it one line\n",
    "walden_raw_oneline = walden_raw.replace('\\n', ' ')\n",
    "\n",
    "####################\n",
    "## Regex examples ##\n",
    "####################\n",
    "\n",
    "## Extract every 4-digit number in Walden (likely years)\n",
    "\n",
    "# Pattern\n",
    "# -------> In regex, \"\\d\" matches any digit, so '\\d\\d\\d\\d' means a string of four digits\n",
    "four_digit = '\\d\\d\\d\\d'\n",
    "\n",
    "# Find all matches for the 'four_digit' pattern in 'walden_raw_oneline'\n",
    "four_digit_result = re.findall(four_digit, walden_raw_oneline) #print(result)\n",
    "\n",
    "# Print the list of matches as a set, to remove duplicates\n",
    "print( \"Four digits:\", set( four_digit_result ) )\n",
    "\n",
    "\n",
    "## Extract all capitalized words\n",
    "\n",
    "# Pattern\n",
    "# -------> In regex, \"[A-Z] matches all capitalized words and [a-z] all lowercase. '*' matches everthing!\n",
    "capitalized = '[A-Z][a-z]*'\n",
    "\n",
    "# Find all matches for the 'capitalized' pattern in 'walden_raw_oneline'\n",
    "capitalized_result = re.findall(capitalized, walden_raw_oneline)\n",
    "\n",
    "# Print the list of matches as a set, to remove duplicates\n",
    "print( \"\\nCapitalized:\", set( capitalized_result ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it's time to use regex to help us do part of speech tagging!\n",
    "\n",
    ">**In the next code block, we create another version of the data that preserves (informative) capitalization. That means we will transform the first letter of every sentence into lowercase, preserving non-initial capitalization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "## Make sentence-initial characters lowercase ##\n",
    "################################################\n",
    "\n",
    "# Copy the one-line version of Walden into the variable 'walden_raw_caps'\n",
    "walden_raw_caps = walden_raw_oneline\n",
    "\n",
    "# We prepare regex for the following rule\n",
    "#    '[.!?]' looks for expressions that begin with these punctuation marks\n",
    "#    '\\ +'   looks for one or more spaces.\n",
    "#                 The '\\' symbol \"escapes\" the space behind it...\n",
    "#                      ...telling regex to treat the space as a character to search for.\n",
    "#    '[A-Z]' looks for any capitalized character\n",
    "p = re.compile( \"[.!?]\\ +[A-Z]\" )\n",
    "\n",
    "# Loop every regex match in 'walden_raw_oneline'\n",
    "for match in p.finditer( walden_raw_oneline ):\n",
    "    \n",
    "    # Replace each match in 'walden_raw_caps' with its lowercase version as soon as we find it\n",
    "    walden_raw_caps = walden_raw_caps.replace( match.group(), match.group().lower() )\n",
    "\n",
    "# Print the original raw text (on one line) and the version...\n",
    "#      ...with sentence initial lowercase characters (walden_raw_caps)\n",
    "print( \"^Original:\", walden_raw.replace('\\n', ' ')[:1000] )\n",
    "print( \"\\n^Replacement:\", walden_raw_caps[:1000] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Now that the first letter of every sentence had been made lowercase, capitalization should be a better indication that a token is a proper noun.**\n",
    "\n",
    "> In the next code bock, we repeat the steps we took before with the all-lowercase text, from cleaning to tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "## Tokenize, clean, and tag Walden from scratch ##\n",
    "##################################################\n",
    "\n",
    "# Tokenize the raw text\n",
    "walden_tokens_caps = word_tokenize( walden_raw_caps )\n",
    "\n",
    "# Remove nonalphabetical tokens\n",
    "walden_tokens_caps = [ token for token in walden_tokens_caps if token.isalpha() ]\n",
    "\n",
    "# Remove stopword tokens\n",
    "walden_tokens_caps = [token for token in walden_tokens_caps if not token in set(stopwords.words('english'))]\n",
    "\n",
    "# We will NOT change capitalization, to see whether that changes our named entity list\n",
    "\n",
    "# Tag each token with its part of speech\n",
    "walden_tagged_caps = pos_tag( walden_tokens_caps )\n",
    "\n",
    "# Print the list of tokens and their tag\n",
    "walden_tagged_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "## Print every token tagged as a proper noun ##\n",
    "###############################################\n",
    "\n",
    "# Exactly as before, we loop every tag and print the word when its tag matches \"NNP\"\n",
    "for i in range(0, len(walden_tagged_caps)):\n",
    "    if walden_tagged_caps[i][1] == 'NNP':\n",
    "        print( walden_tagged_caps[i] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Plot the distribution of parts of speech again ##\n",
    "####################################################\n",
    "\n",
    "# Take only the tags (word is index 0, part of speech tag is index 1)\n",
    "walden_tags_caps = [x[1] for x in walden_tagged_caps]\n",
    "\n",
    "# Convert the list of POS tags to a pandas series for easy plotting\n",
    "walden_tags_caps = pd.Series( walden_tags_caps )\n",
    "\n",
    "# Create a bar plot of the counts of each tag (= histogram)\n",
    "walden_tags_caps_plot_updated = walden_tags_caps.value_counts().plot('bar')\n",
    "\n",
    "# Let's also add a title to our plot\n",
    "plt.title('POS Frequency in Walden (updated)')\n",
    "\n",
    "# Show the plot\n",
    "walden_tags_caps_plot_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks good! There are a lot of proper nouns, *but* also a lot of capitalized words. It looks like this part of speech tagger does not do too well in identifying proper nouns.**\n",
    "\n",
    "Next we will look at a very different way to look up the part of speech of a word.\n",
    "\n",
    "## WordNet\n",
    "\n",
    "NLTK's default part of speech tagger uses capitalization to guess proper nouns, but there \n",
    "\n",
    "\n",
    "\n",
    "Instead of using capitalization and context to try to label each part of speech, let's look each word up in a dictionary. WordNet is a kind of dictionary that comes with NLTK that helps look up synonymns, definitions, and parts of speech (one per definition).\n",
    "\n",
    "> In the next code chunk, we create a list of all of the unique words in Walden and look them up one by one, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "## Tag each token with its most frequent 'dictionary' part of speech ##\n",
    "#######################################################################\n",
    "\n",
    "# Import WordNet from NLTK\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Collapse the cleaned tokens into a set, which means every element will be unique\n",
    "walden_tokens_unique = set( walden_tokens )\n",
    "\n",
    "# Empty list which will hold each unique token and its part of speech\n",
    "walden_tagged_wordnet = []\n",
    "\n",
    "# Loop over every unique token\n",
    "for token in walden_tokens_unique:\n",
    "    \n",
    "    # Look up the token in WordNet and extract its part of speech, using the pos() function\n",
    "    # Returns a list of parts of speech, one for every definition\n",
    "    token_pos = [ss.pos() for ss in wordnet.synsets( token )]\n",
    "    \n",
    "    # If we cannot find any definitions for the token\n",
    "    # % why greater than 1?\n",
    "    if len(token_pos) > 0:\n",
    "\n",
    "        # We take only the part of speech that occurs the most\n",
    "        token_pos = max(set( token_pos ), key = token_pos.count)\n",
    "        \n",
    "        # We count the occurances of the token in walden_tokens (a list)\n",
    "        token_freq = walden_tokens.count(token)\n",
    "\n",
    "        # Update the dictionary list with a list of information for this token\n",
    "        # A list of lists!\n",
    "        walden_tagged_wordnet.append( [token, token_pos, token_freq] )\n",
    "        \n",
    "        # Print the token, its part of speech, and its frequency as we go\n",
    "        print( token, token_pos, token_freq )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It looks like the part of speech tagger did a great job, even without capitalization.\n",
    "\n",
    "**Next we will compare these results to the default nltk part of speech tagger.**\n",
    "\n",
    ">The next block shows the part of speech frequency distribution from both tagging methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "## Plot the frequency distribution of the parts of speech for both methods ##\n",
    "#############################################################################\n",
    "\n",
    "# Take only the tags (word is index 0, part of speech tag is index 1)\n",
    "walden_tags_wordnet = [x[1] for x in walden_tagged_wordnet]\n",
    "\n",
    "# Convert the list of POS tags to a pandas series for easy plotting\n",
    "walden_tags_wordnet = pd.Series( walden_tags_wordnet )\n",
    "\n",
    "# Create a blank figure and axes to hold our subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "# Subplot 1 (left)\n",
    "walden_tags.value_counts().plot(ax=axes[0], kind='bar').title.set_text('NLTK default (Porter)')\n",
    "\n",
    "# Subplot 2 (right)\n",
    "walden_tags_wordnet.value_counts().plot(ax=axes[1], kind='bar').title.set_text('NLTK WordNet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see there are differences between both part of speech tagging methods.\n",
    "\n",
    "How would the graph change if we used `walden_stems` instead of `walden_tokens`? Change the code to evaluate your hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks!\n",
    "\n",
    "I hope this helped clarify how usful nltk (still) can be for an NLP pipeline!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to get more information on nltk\n",
    "\n",
    "Parts of this workshop were inspited by existing tutorials, classes, and books, which are:\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.nltk.org/book/</li>\n",
    "    <li>http://www.ling.helsinki.fi/kit/2009s/clt231/NLTK/book/ch01-LanguageProcessingAndPython.html#sec-computing-with-language-simple-statistics</li>\n",
    "    <li>https://library.nd.edu/event/introduction-to-python-and-the-nltk-2019-11-19</li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "You are encouraged to check out these resources and explore the many others that have been posted online. There's a lot out there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
