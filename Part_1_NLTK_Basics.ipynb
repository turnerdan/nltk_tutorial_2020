{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Intro to NLTK!\n",
    "\n",
    "**This workshop will walk you through the common functions of NLTK in Python!**\n",
    "\n",
    "Run each chunk in top-to-bottom order and follow along with the code.\n",
    "\n",
    "I am assuming minimal experience with Python3 and programming in general.\n",
    "\n",
    "### *Packaged Texts.*\n",
    "There are two texts packaged with this workshop that provide different levels of challenges: **(1)** Walden by Henry David Thoreau and **(2)** Complete Works of William Shakespeare.\n",
    "\n",
    "These were downloaded from Project Gutenberg (https://www.gutenberg.org), a website that hosts a library of over 60,000 free books. Many of these books are available in plaintext (.txt) format, making them easilly parsable by packages like NLTK. That said, the skills from this workshop are broadly applicable to any type and format of machine-readable text, not just books from Project Gutenburg and not just plaintext.\n",
    "\n",
    "\n",
    "## Installing Required Packages\n",
    "First we will load NLTK into the environment and make sure we can parse text files.\n",
    "\n",
    "### *Installing NLTK.* \n",
    "NLTK is preinstalled on this virtual server, so if you follow this workshop on your own machine, then you will have to install the package on your own. In most cases, this process only takes one line of code and a few minutes. You can find instructions here: https://www.nltk.org/install.html\n",
    "\n",
    "### *Installing other packages.*\n",
    "Other packages required in this workshop are **numpy**, **pickle**, **os**, **re**, **string**, **scipy**, and **pyplot**. Like NLTK, they are preinstalled on this server, but if you want to run these scripts on a different machine, you will need to install these packages.\n",
    "\n",
    "\n",
    "## First Code Chunk: Load NLTK and download popular NLTK packages\n",
    "The following box is an example of a code chunk! Run it to import (= load) NLTK into the environment. It will take a few minutes and you will see an asterisk (`*`) to the left of the block while it runs.\n",
    "\n",
    "To run a chunk, you can press the \"Run\" button above, or hold Shift and press Return/Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## Set up NLTK ##\n",
    "#################\n",
    "\n",
    "# Import all functions from NLTK\n",
    "# This allows us to call nltk functions without specifying the package\n",
    "from nltk import *\n",
    "\n",
    "# Download or update popular NLTK packages\n",
    "# This will let us filter punctuation and common keywords (i.e. stopwords)\n",
    "download('popular')\n",
    "\n",
    "# This will allow us to tag tokens in our texts in different ways, including by part of speech\n",
    "download('tagsets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our connection to a text file\n",
    "\n",
    "This code chunk loads the OS package which will help us to load external files, including our texts.\n",
    "\n",
    "Specifically, we will load the famous American novel 'Walden' into the variable 'file' and use a \"for loop\" to print the first 20 lines of the novel. This is not a necessary step, but it is a good idea to test the connection to your text file, since problems with file paths are common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy import special\n",
    "from nltk import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# The relative path to our text file.\n",
    "# In other words, where the file is relative to this Jupyter notebook.\n",
    "pathToFile = 'texts/walden.txt'\n",
    "\n",
    "# Open Walden and read it (hence the 'r') into the variable 'file'\n",
    "file = open( pathToFile, 'r')\n",
    "\n",
    "# This \"for loop\" will repeat 20 times -- that is what the range() function is doing\n",
    "# Inside the loop, readline() prints the *next* line, relative to the previous loop.\n",
    "# So the two lines do a lot. It counts from 0 to 20 and for each count...\n",
    "for x in range(20):\n",
    "    \n",
    "    # ...it prints the next line from our text file\n",
    "    print( file.readline() )\n",
    "\n",
    "# This helps matlabplot and jupyter cooperate in showing graphics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use Python (versus other languages)?\n",
    "**In my experience, people tend to prefer Python for text processing and analysis over other programming languages like R, Ruby, Matlab, and others.**\n",
    "\n",
    "While programming languages are designed to be general purpose, there are more packages and resources for text-related tasks than practically any other language. This means that if you want to do some online data mining, work with gigantic corpora, or custom semantic tagging systems (say, for happy versus sad words), you can do all of this *and* the analysis in the same language. Take the example of data mining. You could:\n",
    "\n",
    "<ol>\n",
    "    <li>Use the <code>BeautifulSoup</code> package to scrape a list of webpages and parse them into plaintext.</li>\n",
    "    <li>With regular expressions (regex), <code>nltk</code>, and the <code>string</code> package you can clean the plaintext, tokenize it, create frequency distributions and ngram models.</li>\n",
    "    <li><code>pandas</code> is good for data analyis tasks for the statistics that come out of NLTK.</li>\n",
    "    <li>You can even train a \"deep learning\" neural network based on text data without leaving Python, thanks to <code>PyTorch</code></li>\n",
    "</ol>\n",
    "\n",
    "There's also the fact that Python is widely known compared to R and Matlab, which are other languages that can do \"all of the above\" when it comes to text analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "**In this Python script, we will clean the data.**\n",
    "\n",
    "This means we will *filter* and *transform* the data to include only the elements of interest, and we will return it in a format that makes it easy to analyze later.\n",
    "\n",
    "**Goals**\n",
    "<ol>\n",
    "    <li> Tokenize the text into words </li>\n",
    "    <li> Remove punctuation and uninformative words </li>\n",
    "    <li> Convert all words to lowercase </li>\n",
    "    <li> Read a frequency distribution </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text\n",
    "\n",
    "Tokenization is the process of breaking down a big text into units of interest, usually words.\n",
    "\n",
    "But, what is a word?  This might seem like a weird question, but consider the example \"black sheep\", defined by the Cambridge English Dictionary below:\n",
    "\n",
    ">*Someone who embarrasses a group or family because the person is different or has gotten into trouble.*\n",
    "\n",
    "Based on this, you might say that the 'sheep' in 'black sheep' is different than the 'sheep' in the phrase 'my pet sheep', and you have the beginnings of a good argument that 'black sheep' is one word, not two.\n",
    "\n",
    "> NLTK (and most text analytics tools) will tokenize \"my black sheep uncle owns a black cat\" as `['my', 'black', 'sheep', 'uncle', 'owns', 'a', 'black', 'cat']`. **We have a hunch that this is wrong, but whether it matters for your analysis is up to you.**\n",
    "\n",
    "For the purposes of today, we will ignore these kinds of issues, but you should keep in mind that NLTK is built to work for most purposes, not necessarilly your purposes. Before you begin any text analysis project or experiment, you must first define exactly what kind of elements you are interested in, so you can identify those in continuous text.\n",
    "\n",
    "Generally, without more information, a tokenizer will usually split a text into tokens based on spaces and punctuation, a strategy that works for most words.\n",
    "\n",
    ">**Run the code chunk below to see how NLTK tokenizes Walden using its default settings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "## Read-in and tokenize Walden ##\n",
    "#################################\n",
    "\n",
    "# Read the Walden file\n",
    "walden_raw = file.read( )\n",
    "\n",
    "# Tokenize 'text' with the NLTK function word_tokenize() and save the result to the variable 'tokens'\n",
    "tokens = word_tokenize( walden_raw )\n",
    "\n",
    "# Print the result of the tokenization\n",
    "print( tokens )\n",
    "\n",
    "# Close the Walden text file to release it from memory\n",
    "# We don't need the original text anymore.\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the tokenization\n",
    "\n",
    "Take a minute to look through the tokens that NLTK found. Is there anything there that might get in the way of an analysis of the text?\n",
    "\n",
    "\n",
    "## Filtering non-alphabetical characters\n",
    "\n",
    "Filtering in NLTK usually entails comparing elements between lists. For example, if we want to see whether a particular token (usually equivalent to the idea of word) includes characters outside of the alphabet, we would compare each character to a list of all the letters in the alphabet (uppercase and lowercase).\n",
    "\n",
    "This is the function of NLTK's `isalpha()`.\n",
    "\n",
    ">**In the next code chunk we find the 100 most frequent tokens that include characters outside of the alphabet.** This will help us determine whether we should filter non-alphabetical characters. Follow along with the comments in the code to learn more about each step of the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "## Find tokens with characters that are not A-Z ##\n",
    "##################################################\n",
    "\n",
    "# Compare each token in Walden to the alphabet, returning when the token includes non-alphabetical characters\n",
    "notalpha = [ token for token in tokens if not token.isalpha() ]\n",
    "\n",
    "# Count the frequency of each token.\n",
    "# Answers the question, How many times does each token appear across the corpus?\n",
    "# Save the result to the 'freqs' variable\n",
    "freqs = FreqDist(notalpha)\n",
    "\n",
    "# Print the 100 most frequent tokens with non-alphabetical characters\n",
    "print( freqs.most_common(100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, should we apply the filter to remove tokens with non-alphabetical characters?\n",
    "\n",
    "**It looks like there are a few kinds of tokens here**, all of which would be removed from our dataset if we filtered using `isalpha()`:\n",
    "<ul>\n",
    "    <li> Punctuation (i.e. commas, which appear 8484 times) </li>\n",
    "    <li> Numbers (i.e. '1.73', which appears twice) </li>\n",
    "    <li> Hypenated words (i.e. 'so-called', which appears 7 times) </li>\n",
    "</ul>\n",
    "\n",
    "For the purposes of this workshop, let's assume we want to remove all of these tokens from our data. But you might want to give a little bit of thought about how we can possibly filter the text before tokenizing it to save more tokens from unnecessary filtering.\n",
    "\n",
    ">**The next code chunk removes tokens that include non-alphabetical characters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Remove tokens with characters that are not A-Z ##\n",
    "####################################################\n",
    "\n",
    "# Read the list of tokens and only return each token if it's all alphabetical\n",
    "# We write the result to a new variable, tokens_clean, which we will overwrite until it's cleaned.\n",
    "tokens_clean = [ token for token in tokens if token.isalpha() ]\n",
    "\n",
    "\n",
    "print(tokens_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text to lowercase\n",
    "\n",
    "**NLTK considers \"Amber\" and \"amber\" to be different tokens.** While it might be useful to make a distinction between these tokens (for example, for identifying some proper names), usually we choose to make all characters lowercase.\n",
    "\n",
    "We can convert all of the text into lowercase by passing each token using the `lower()` function. There are more nuanced ways to do this, such as by separating capitalization at the beginning of sentences versus within sentences, that might be more appropriate for your data. You will see an example of this selective transformat\n",
    "\n",
    ">**For our puposes, we will just convert all tokens to lowercase in the next code chunk.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "## Convert tokens to lowercase ##\n",
    "#################################\n",
    "\n",
    "# Use another list comprehension to save the lowercase version to tokens_clean (overwriting the original)\n",
    "tokens_clean = [ token.lower() for token in tokens_clean ]\n",
    "\n",
    "# Print the result\n",
    "print( tokens_clean )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distributions\n",
    "\n",
    "The most common word in English is \"the\", which makes it not very informative. Words like this are known as \"stopwords\", and removing them from a corpus is a normal part of text analysis. Before we remove the stopwords, let's look at the most common words in Walden and see how stopwords might obscure more important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Generate a token frequency distribution ##\n",
    "#############################################\n",
    "\n",
    "# This line specifies the size of the figure as out Jupyter Notebook will print it\n",
    "plt.rcParams[\"figure.figsize\"] = [16,7]\n",
    "\n",
    "# Calculate the frequency for each token from the book and save it to the variable 'frequencies'\n",
    "frequencies = FreqDist( tokens_clean )\n",
    "\n",
    "# Call 'frequencies' with the method 'plot' to generate a frequency plot of the 50 most frequent words\n",
    "walden_plot = frequencies.plot( 50 )\n",
    "\n",
    "# The line above saves the plot to the variable 'walden_plot'. This line outputs it, so it will appear below.\n",
    "walden_plot\n",
    "\n",
    "# Also print the raw list\n",
    "frequencies.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords\n",
    "\n",
    "One implication for this is that **most of the tokens in our dataset are not informative** because most of the high-frequency tokens in Walden are high frequency in every other English text. If we want to learn more about Walden, then we need to focus on lower frequency tokens.\n",
    "\n",
    "In other words, if we followed every step in this tutorial so far with a different text, the result would likely be exactly the same. If we are asking questions about English in general this might be useful, but instead of settling for this general dataset, we will remove the most common tokens to make our data more representative of Walden.\n",
    "\n",
    "Luckily, NLTK has many premade lists of very common tokens, and we already installed the lists! If you want to see it, just run the line `set(stopwords.words('english'))`.\n",
    "\n",
    ">**The next code block removes stopwords. This step looks a lot like filtering we did on non-alphabetical tokens.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## Remove stopwords ##\n",
    "######################\n",
    "\n",
    "# Bigger figure\n",
    "plt.rcParams[\"figure.figsize\"] = [16,7]\n",
    "\n",
    "# Check every token in 'tokens_clean' against the NLTK stopword list\n",
    "# Only keep tokens NOT in the list\n",
    "tokens_clean = [t for t in tokens_clean if not t in set(stopwords.words('english'))]\n",
    "\n",
    "# Generate a new frequency distribution plot with the 50 most frequent words remaining after filtering\n",
    "# This follows the same steps as before: We count the frequency of each token, we generate a plot of the top 50, and then we display them.\n",
    "frequencies = FreqDist( tokens_clean )\n",
    "\n",
    "# Make plot with top 50 tokens\n",
    "walden_plot = frequencies.plot( 50 )\n",
    "\n",
    "# Show the plot\n",
    "walden_plot\n",
    "\n",
    "# Also print the raw list\n",
    "frequencies.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning is time consuming and imperfect\n",
    "\n",
    "**As you can tell from the words in the plot above, we now have a fairly clean and informative data set!**\n",
    "\n",
    "For example, we can tell just by looking at this that Walden may have a lot to do with the natural world (water, pond, ice, winter, nature, world) and was perhaps about a man living in nature (man, men, time, house).\n",
    "\n",
    "Of course, there are other frequent tokens that might provide less information for your analysis (would, may, though, two, us). You can either filter these words out using *lists*, as we did above for non-alphabetical tokens and for stopwords, *or* you can filter our entire classes of words (i.e. pronouns) from your analysis. This will be detailed in the next section.\n",
    "\n",
    "First we will save our progress in the **pickle format**, which is an efficient way to store raw data in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## Save our progress ##\n",
    "#######################\n",
    "\n",
    "# Write the tokens list to the /working/ folder\n",
    "with open('working/walden_clean_tokens.pkl', 'wb') as f:\n",
    "    pickle.dump( tokens_clean, f )\n",
    "\n",
    "# Also write a Text object (unique to NLTK) to the /working/ folder \n",
    "with open('working/walden_text.pkl', 'wb') as f:\n",
    "    pickle.dump( Text( tokens ), f )\n",
    "    \n",
    "# Also write the text to the /working/ folder \n",
    "with open('working/walden_raw.pkl', 'wb') as f:\n",
    "    pickle.dump( walden_raw, f )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
